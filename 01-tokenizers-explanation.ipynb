{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/adalbertii/modele-NLP/blob/main/01-tokenizers-explanation.ipynb",
      "authorship_tag": "ABX9TyPWymUi1WTPByGvcWZ9l413",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Tokenizacja za pomocą TF Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Przegląd\n",
        "\n",
        "Tokenizacja to proces dzielenia ciągu znaków na tokeny. Zazwyczaj tokeny te to słowa, liczby i/lub znaki interpunkcyjne. Pakiet `tensorflow_text` udostępnia szereg tokenizerów do wstępnego przetwarzania tekstu wymaganego przez modele tekstowe. Wykonując tokenizację w grafie TensorFlow, nie trzeba martwić się o różnice między przepływami pracy uczenia i wnioskowania oraz zarządzaniem skryptami wstępnego przetwarzania.\n",
        "\n",
        "W tym Notebook-u omówiono wiele opcji tokenizacji udostępnianych przez TensorFlow Text, sytuacje, w których warto użyć jednej opcji zamiast drugiej oraz sposób wywoływania tokenizatorów z poziomu modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Konfiguracja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z0oj4HS26x05",
        "outputId": "7b3b2b18-04b0-4598-b6a2-538fdf14a740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q \"tensorflow-text==2.11.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "alf2kDHJ60rO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4rfKxVvBBu0"
      },
      "source": [
        "## Splitter API\n",
        "\n",
        "Głównymi interfejsami są `Splitter` i `SplitterWithOffsets`, które posiadają pojedyncze metody `split` i `split_with_offsets`. Wariant `SplitterWithOffsets` (który rozszerza `Splitter`) zawiera opcję pobierania przesunięć bajtów. Pozwala to wywołującemu dowiedzieć się, z których bajtów oryginalnego łańcucha został utworzony token.\n",
        "\n",
        "`Tokenizer` i `TokenizerWithOffsets` są wyspecjalizowanymi wersjami `Splitter`, które dostarczają wygodnych metod odpowiednio `tokenize` i `tokenize_with_offsets`.\n",
        "\n",
        "Ogólnie rzecz biorąc, dla dowolnego N-wymiarowego wejścia, zwrócone tokeny są w N+1-wymiarowym [RaggedTensor](https://www.tensorflow.org/guide/ragged_tensor) z najbardziej wewnętrznym wymiarem tokenów mapującym do oryginalnych pojedynczych łańcuchów.\n",
        "\n",
        "```python\n",
        "class Splitter {\n",
        "  @abstractmethod\n",
        "  def split(self, input)\n",
        "}\n",
        "\n",
        "class SplitterWithOffsets(Splitter) {\n",
        "  @abstractmethod\n",
        "  def split_with_offsets(self, input)\n",
        "}\n",
        "```\n",
        "\n",
        "Istnieje również interfejs `Detokenizer`. Każdy tokenizer implementujący ten interfejs może zaakceptować N-wymiarowy poszarpany tensor tokenów i zwykle zwraca N-1-wymiarowy tensor lub poszarpany tensor, który ma podane tokeny złożone razem.\n",
        "\n",
        "```python\n",
        "class Detokenizer {\n",
        "  @abstractmethod\n",
        "  def detokenize(self, input)\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhviJXy0BDoa"
      },
      "source": [
        "## Tokenizatory\n",
        "\n",
        "Poniżej znajduje się zestaw tokenizerów dostarczanych przez TensorFlow Text. Zakłada się, że ciągi wejściowe są w formacie UTF-8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWFisXk-68BQ"
      },
      "source": [
        "### Tokenizatory całych słów\n",
        "\n",
        "These tokenizers attempt to split a string by words, and is the most intuitive way to split text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CxjAs5wOYKh"
      },
      "source": [
        "#### WhitespaceTokenizer\n",
        "\n",
        "Text.WhitespaceTokenizer` jest najbardziej podstawowym tokenizerem, który rozdziela ciągi znaków na zdefiniowane przez ICU białe znaki (np. spacja, tabulator, nowa linia). Jest to często dobre rozwiązanie do szybkiego budowania prototypowych modeli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k4a11Hlm7C4k",
        "outputId": "1258d3ab-6a60-4358-8bf1-326555396347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHS6dEQ7cR9E"
      },
      "source": [
        "Można zauważyć, że wadą tego tokenizera jest to, że interpunkcja jest dołączana do słowa, aby utworzyć token. Aby rozdzielić słowa i interpunkcję na osobne tokeny, należy użyć `UnicodeScriptTokenizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xohhm0Q7AmN"
      },
      "source": [
        "#### UnicodeScriptTokenizer\n",
        "\n",
        "Funkcja `UnicodeScriptTokenizer` dzieli ciągi znaków w oparciu o granice skryptów Unicode. Używane kody skryptów odpowiadają wartościom UScriptCode International Components for Unicode (ICU). ( http://icu-project.org/apiref/icu4c/uscript_8h.html)\n",
        "\n",
        "W praktyce jest to podobne do `WhitespaceTokenizer` z najbardziej widoczną różnicą polegającą na tym, że rozdziela interpunkcję (USCRIPT_COMMON) od tekstów językowych (np. USCRIPT_LATIN, USCRIPT_CYRILLIC, itp.), jednocześnie oddzielając teksty językowe od siebie. Należy pamiętać, że spowoduje to również rozdzielenie wyrazów skracanych na osobne tokeny."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "68u0XF3L6-ay",
        "outputId": "649b87b8-9f1c-49ec-9e3c-19a70434ccc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b'can', b\"'\", b't', b'explain', b',', b'but', b'you', b'feel', b'it', b'.']]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Ja_h1qO7P0"
      },
      "source": [
        "### Tokenizatory podsłów\n",
        "\n",
        "Subword tokenizers can be used with a smaller vocabulary, and allow the model to have some information about novel words from the subwords that make create it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLif2owYPBos"
      },
      "source": [
        "#### WordpieceTokenizer\n",
        "\n",
        "Tokenizacja WordPiece to oparty na danych schemat tokenizacji, który generuje zestaw tokenów podrzędnych.\n",
        "\n",
        "WordpieceTokenizer oczekuje, że dane wejściowe są już podzielone na tokeny. Z powodu tego warunku wstępnego, często będziesz chciał wcześniej dokonać podziału za pomocą `WhitespaceTokenizer` lub `UnicodeScriptTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "srIHtzU2fxCi",
        "outputId": "794e260b-5808-45d7-d825-c4ddf905adc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUZe66RngCGU"
      },
      "source": [
        "Po podzieleniu łańcucha na tokeny, `WordpieceTokenizer` może zostać użyty do podzielenia go na podtokeny."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ISEUjIsYAl2S",
        "outputId": "fc4d3d62-8bd0-49ed-a4ca-98ed5be29b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52382"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_wp_en_vocab.txt?raw=true\"\n",
        "r = requests.get(url)\n",
        "filepath = \"vocab.txt\"\n",
        "open(filepath, 'wb').write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uU8wJlUfsskU",
        "outputId": "367f3a66-e09c-47c3-8e9c-2d408721365b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[b'What'], [b'you'], [b'know'], [b'you'], [b\"can't\"], [b'explain,'], [b'but'], [b'you'], [b'feel'], [b'it.']]]\n"
          ]
        }
      ],
      "source": [
        "subtokenizer = tf_text.UnicodeScriptTokenizer(filepath)\n",
        "subtokens = tokenizer.tokenize(tokens)\n",
        "print(subtokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncBcigHAPFBd"
      },
      "source": [
        "#### BertTokenizer\n",
        "\n",
        "BertTokenizer odzwierciedla oryginalną implementację tokenizacji z artykułu BERT. Jest on wspierany przez WordpieceTokenizer, ale wykonuje również dodatkowe zadania, takie jak normalizacja i tokenizacja słów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2tOz1hNhtdV2",
        "outputId": "dd24162d-a9a0-458d-d4e3-5d2ec45d9473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[b'what'], [b'you'], [b'know'], [b'you'], [b'can'], [b\"'\"], [b't'], [b'explain'], [b','], [b'but'], [b'you'], [b'feel'], [b'it'], [b'.']]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.BertTokenizer(filepath, token_out_type=tf.string, lower_case=True)\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rb_dORMO-3t"
      },
      "source": [
        "#### SentencepieceTokenizer\n",
        "\n",
        "SentencepieceTokenizer to tokenizator sub-tokenów, który jest wysoce konfigurowalny. Jest on wspierany przez bibliotekę Sentencepiece. Podobnie jak BertTokenizer, może obejmować normalizację i podział tokenów przed podziałem na tokeny podrzędne.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0dUbFCfDCojr"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/tensorflow/text/blob/master/tensorflow_text/python/ops/test_data/test_oss_model.model?raw=true\"\n",
        "sp_model = requests.get(url).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uvsm6iuNupEZ",
        "outputId": "7935f581-f2a9-489d-a5a7-59dc558e6bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'\\xe2\\x96\\x81What', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81know', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81can', b\"'\", b't', b'\\xe2\\x96\\x81explain', b',', b'\\xe2\\x96\\x81but', b'\\xe2\\x96\\x81you', b'\\xe2\\x96\\x81feel', b'\\xe2\\x96\\x81it', b'.']]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.SentencepieceTokenizer(sp_model, out_type=tf.string)\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TatehW0Q0qV"
      },
      "source": [
        "### Inne splitters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqNgtoFPQ1sG"
      },
      "source": [
        "#### UnicodeCharTokenizer\n",
        "\n",
        "Dzieli ciąg znaków na znaki UTF-8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4GjiAnQFvIhW",
        "outputId": "b7a835c1-10c2-4dc3-b8e8-5a5f85c737ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.UnicodeCharTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHyWQcJZGOwL"
      },
      "source": [
        "Wynikiem są liczby kodowe Unicode. Może to być również przydatne do tworzenia ngramów znaków, takich jak bigramy. Aby przekonwertować z powrotem na znaki UTF-8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_uuyz3XC0NdU",
        "outputId": "3df45311-16ef-4f6c-f50f-b09afe6ae044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'Wh', b'ha', b'at', b't ', b' y', b'yo', b'ou', b'u ', b' k', b'kn', b'no', b'ow', b'w ', b' y', b'yo', b'ou', b'u ', b' c', b'ca', b'an', b\"n'\", b\"'t\", b't ', b' e', b'ex', b'xp', b'pl', b'la', b'ai', b'in', b'n,', b', ', b' b', b'bu', b'ut', b't ', b' y', b'yo', b'ou', b'u ', b' f', b'fe', b'ee', b'el', b'l ', b' i', b'it', b't.']]\n"
          ]
        }
      ],
      "source": [
        "characters = tf.strings.unicode_encode(tf.expand_dims(tokens, -1), \"UTF-8\")\n",
        "bigrams = tf_text.ngrams(characters, 2, reduction_type=tf_text.Reduction.STRING_JOIN, string_separator='')\n",
        "print(bigrams.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWrGTOzbVb8U"
      },
      "source": [
        "#### RegexSplitter\n",
        "\n",
        "RegexSplitter jest w stanie segmentować ciągi znaków w dowolnych punktach przerwania zdefiniowanych przez podane wyrażenie regularne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Szw0QZ6ecExC",
        "outputId": "af2c3fd5-3833-414f-fc10-bc196fea59a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'What', b'you', b'know', b'you', b\"can't\", b'explain,', b'but', b'you', b'feel', b'it.']]\n"
          ]
        }
      ],
      "source": [
        "splitter = tf_text.RegexSplitter(\"\\s\")\n",
        "tokens = splitter.split([\"What you know you can't explain, but you feel it.\"], )\n",
        "print(tokens.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPIMvyot7GFv"
      },
      "source": [
        "## Offsets\n",
        "\n",
        "Podczas tokenizacji ciągów znaków, często pożądane jest, aby wiedzieć, skąd w oryginalnym ciągu znaków pochodzi token. Z tego powodu każdy tokenizator, który implementuje `TokenizerWithOffsets` posiada metodę *tokenize_with_offsets*, która zwróci przesunięcia bajtów wraz z tokenami. Start_offsets zawiera listę bajtów w oryginalnym łańcuchu, od których zaczyna się każdy token, a end_offsets zawiera listę bajtów bezpośrednio po punkcie, w którym kończy się każdy token. Aby uściślić, przesunięcia początkowe są włącznie, a przesunięcia końcowe są wyłączne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UmZ91zl87J7y",
        "outputId": "252e061a-15e8-464a-c0cc-a8ec25d948d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost', b'.']]\n",
            "[[0, 11, 15, 21, 26, 29, 33]]\n",
            "[[10, 14, 20, 25, 28, 33, 34]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "(tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(['Everything not saved will be lost.'])\n",
        "print(tokens.to_list())\n",
        "print(start_offsets.to_list())\n",
        "print(end_offsets.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVGbkB-80819"
      },
      "source": [
        "## Detokenization\n",
        "\n",
        "Tokenizery implementujące `Detokenizer` zapewniają metodę `detokenize`, która próbuje połączyć ciągi. Może to być stratne, więc zdetokenizowany ciąg może nie zawsze dokładnie odpowiadać oryginalnemu, wstępnie ztokenizowanemu ciągowi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iyThnPPQ0_6Q",
        "outputId": "99e20206-d852-401a-eb43-088c24a25e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[87, 104, 97, 116, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 121, 111, 117, 32, 99, 97, 110, 39, 116, 32, 101, 120, 112, 108, 97, 105, 110, 44, 32, 98, 117, 116, 32, 121, 111, 117, 32, 102, 101, 101, 108, 32, 105, 116, 46]]\n",
            "[b\"What you know you can't explain, but you feel it.\"]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tf_text.UnicodeCharTokenizer()\n",
        "tokens = tokenizer.tokenize([\"What you know you can't explain, but you feel it.\"])\n",
        "print(tokens.to_list())\n",
        "strings = tokenizer.detokenize(tokens)\n",
        "print(strings.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVNFPYSZ7sf1"
      },
      "source": [
        "## TF Data\n",
        "\n",
        "TF Data to potężny interfejs API do tworzenia potoku danych wejściowych do trenowania modeli. Tokenizery działają zgodnie z oczekiwaniami z API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YSykDr1d7uxr",
        "outputId": "7313fb31-2b31-4fde-c3fa-799a9427446a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
            "[[b\"It's\", b'a', b'trap!']]\n"
          ]
        }
      ],
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
        "iterator = iter(tokenized_docs)\n",
        "print(next(iterator).to_list())\n",
        "print(next(iterator).to_list())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "tokenizers.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
