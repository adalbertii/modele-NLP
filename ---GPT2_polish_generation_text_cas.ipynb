{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1V6Gk9sA_vine4QcD6bhDIhemQIVMtOB_",
      "authorship_tag": "ABX9TyOprKanKwyYL4oEkekyzHQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalbertii/modele-NLP/blob/main/---GPT2_polish_generation_text_cas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8jM75Q04llx",
        "outputId": "b9837426-108d-4855-b047-0c0027d978ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-21 09:03:42.835159\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "x = datetime.datetime.now()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBLGUlSn481g",
        "outputId": "8cd504e7-da04-4324-ffb6-42a88402a663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.8)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.2)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.25.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.9.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291818 sha256=578d1ade0266dc7393818b04bd7984fa5b66cc2ce0f37b52b05835fc18e20595\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=4c3277ad0be80ac7bf90b59b4f624d1c8d133563e36236434ef6fbcf78b58fa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from fairseq import hub_utils\n",
        "from fairseq.models.transformer_lm import TransformerLanguageModel\n",
        "\n"
      ],
      "metadata": {
        "id": "5QIArPNt4_kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/drive/MyDrive/dane/modele/gpt2_medium_fairseq\"\n",
        "loaded = hub_utils.from_pretrained(\n",
        "    model_name_or_path=model_dir,\n",
        "    checkpoint_file=\"model.pt\",\n",
        "    data_name_or_path=model_dir,\n",
        "    bpe=\"hf_byte_bpe\",\n",
        "    bpe_merges=os.path.join(model_dir, \"merges.txt\"),\n",
        "    bpe_vocab=os.path.join(model_dir, \"vocab.json\"),\n",
        "    load_checkpoint_heads=True,\n",
        "    archive_map=TransformerLanguageModel.hub_models()\n",
        ")\n"
      ],
      "metadata": {
        "id": "6a7e0A1g5Gaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = hub_utils.GeneratorHubInterface(loaded[\"args\"], loaded[\"task\"], loaded[\"models\"])\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkgDu7qE5KIF",
        "outputId": "848b2111-85d9-4b44-fe1a-c64d4a7b0ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorHubInterface(\n",
              "  (models): ModuleList(\n",
              "    (0): TransformerLanguageModel(\n",
              "      (decoder): TransformerDecoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(52004, 1024, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x TransformerDecoderLayerBase(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (output_projection): Linear(in_features=1024, out_features=52004, bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "for idx, m in enumerate(model.modules()):\n",
        "  print(idx, '->', m)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NagSmm5K8Jvt",
        "outputId": "63595008-ba31-45d2-9207-31bb30c119e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 -> GeneratorHubInterface(\n",
            "  (models): ModuleList(\n",
            "    (0): TransformerLanguageModel(\n",
            "      (decoder): TransformerDecoder(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (embed_tokens): Embedding(52004, 1024, padding_idx=1)\n",
            "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "        (layers): ModuleList(\n",
            "          (0-23): 24 x TransformerDecoderLayerBase(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (dropout_module): FairseqDropout()\n",
            "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (activation_dropout_module): FairseqDropout()\n",
            "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (output_projection): Linear(in_features=1024, out_features=52004, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "1 -> ModuleList(\n",
            "  (0): TransformerLanguageModel(\n",
            "    (decoder): TransformerDecoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(52004, 1024, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "      (layers): ModuleList(\n",
            "        (0-23): 24 x TransformerDecoderLayerBase(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (output_projection): Linear(in_features=1024, out_features=52004, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2 -> TransformerLanguageModel(\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(52004, 1024, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=52004, bias=False)\n",
            "  )\n",
            ")\n",
            "3 -> TransformerDecoder(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (embed_tokens): Embedding(52004, 1024, padding_idx=1)\n",
            "  (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "  (layers): ModuleList(\n",
            "    (0-23): 24 x TransformerDecoderLayerBase(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      )\n",
            "      (activation_dropout_module): FairseqDropout()\n",
            "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (output_projection): Linear(in_features=1024, out_features=52004, bias=False)\n",
            ")\n",
            "4 -> FairseqDropout()\n",
            "5 -> Embedding(52004, 1024, padding_idx=1)\n",
            "6 -> LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "7 -> ModuleList(\n",
            "  (0-23): 24 x TransformerDecoderLayerBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (self_attn): MultiheadAttention(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (activation_dropout_module): FairseqDropout()\n",
            "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "8 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "9 -> FairseqDropout()\n",
            "10 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "11 -> FairseqDropout()\n",
            "12 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "13 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "14 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "15 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "16 -> FairseqDropout()\n",
            "17 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "18 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "19 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "20 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "21 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "22 -> FairseqDropout()\n",
            "23 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "24 -> FairseqDropout()\n",
            "25 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "26 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "27 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "28 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "29 -> FairseqDropout()\n",
            "30 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "31 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "32 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "33 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "34 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "35 -> FairseqDropout()\n",
            "36 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "37 -> FairseqDropout()\n",
            "38 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "39 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "40 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "41 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "42 -> FairseqDropout()\n",
            "43 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "44 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "45 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "46 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "47 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "48 -> FairseqDropout()\n",
            "49 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "50 -> FairseqDropout()\n",
            "51 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "52 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "53 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "54 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "55 -> FairseqDropout()\n",
            "56 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "57 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "58 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "59 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "60 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "61 -> FairseqDropout()\n",
            "62 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "63 -> FairseqDropout()\n",
            "64 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "65 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "66 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "67 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "68 -> FairseqDropout()\n",
            "69 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "70 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "71 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "72 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "73 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "74 -> FairseqDropout()\n",
            "75 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "76 -> FairseqDropout()\n",
            "77 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "78 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "79 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "80 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "81 -> FairseqDropout()\n",
            "82 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "83 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "84 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "85 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "86 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "87 -> FairseqDropout()\n",
            "88 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "89 -> FairseqDropout()\n",
            "90 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "91 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "92 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "93 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "94 -> FairseqDropout()\n",
            "95 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "96 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "97 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "98 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "99 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "100 -> FairseqDropout()\n",
            "101 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "102 -> FairseqDropout()\n",
            "103 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "104 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "105 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "106 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "107 -> FairseqDropout()\n",
            "108 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "109 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "110 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "111 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "112 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "113 -> FairseqDropout()\n",
            "114 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "115 -> FairseqDropout()\n",
            "116 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "117 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "118 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "119 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "120 -> FairseqDropout()\n",
            "121 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "122 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "123 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "124 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "125 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "126 -> FairseqDropout()\n",
            "127 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "128 -> FairseqDropout()\n",
            "129 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "130 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "131 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "132 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "133 -> FairseqDropout()\n",
            "134 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "135 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "136 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "137 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "138 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "139 -> FairseqDropout()\n",
            "140 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "141 -> FairseqDropout()\n",
            "142 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "143 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "144 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "145 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "146 -> FairseqDropout()\n",
            "147 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "148 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "149 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "150 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "151 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "152 -> FairseqDropout()\n",
            "153 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "154 -> FairseqDropout()\n",
            "155 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "156 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "157 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "158 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "159 -> FairseqDropout()\n",
            "160 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "161 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "162 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "163 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "164 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "165 -> FairseqDropout()\n",
            "166 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "167 -> FairseqDropout()\n",
            "168 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "169 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "170 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "171 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "172 -> FairseqDropout()\n",
            "173 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "174 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "175 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "176 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "177 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "178 -> FairseqDropout()\n",
            "179 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "180 -> FairseqDropout()\n",
            "181 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "182 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "183 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "184 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "185 -> FairseqDropout()\n",
            "186 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "187 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "188 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "189 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "190 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "191 -> FairseqDropout()\n",
            "192 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "193 -> FairseqDropout()\n",
            "194 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "195 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "196 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "197 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "198 -> FairseqDropout()\n",
            "199 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "200 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "201 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "202 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "203 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "204 -> FairseqDropout()\n",
            "205 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "206 -> FairseqDropout()\n",
            "207 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "208 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "209 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "210 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "211 -> FairseqDropout()\n",
            "212 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "213 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "214 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "215 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "216 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "217 -> FairseqDropout()\n",
            "218 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "219 -> FairseqDropout()\n",
            "220 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "221 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "222 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "223 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "224 -> FairseqDropout()\n",
            "225 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "226 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "227 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "228 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "229 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "230 -> FairseqDropout()\n",
            "231 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "232 -> FairseqDropout()\n",
            "233 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "234 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "235 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "236 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "237 -> FairseqDropout()\n",
            "238 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "239 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "240 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "241 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "242 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "243 -> FairseqDropout()\n",
            "244 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "245 -> FairseqDropout()\n",
            "246 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "247 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "248 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "249 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "250 -> FairseqDropout()\n",
            "251 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "252 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "253 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "254 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "255 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "256 -> FairseqDropout()\n",
            "257 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "258 -> FairseqDropout()\n",
            "259 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "260 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "261 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "262 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "263 -> FairseqDropout()\n",
            "264 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "265 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "266 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "267 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "268 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "269 -> FairseqDropout()\n",
            "270 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "271 -> FairseqDropout()\n",
            "272 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "273 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "274 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "275 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "276 -> FairseqDropout()\n",
            "277 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "278 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "279 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "280 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "281 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "282 -> FairseqDropout()\n",
            "283 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "284 -> FairseqDropout()\n",
            "285 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "286 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "287 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "288 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "289 -> FairseqDropout()\n",
            "290 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "291 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "292 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "293 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "294 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "295 -> FairseqDropout()\n",
            "296 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "297 -> FairseqDropout()\n",
            "298 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "299 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "300 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "301 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "302 -> FairseqDropout()\n",
            "303 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "304 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "305 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "306 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "307 -> TransformerDecoderLayerBase(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (self_attn): MultiheadAttention(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            "  (activation_dropout_module): FairseqDropout()\n",
            "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "308 -> FairseqDropout()\n",
            "309 -> MultiheadAttention(\n",
            "  (dropout_module): FairseqDropout()\n",
            "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            ")\n",
            "310 -> FairseqDropout()\n",
            "311 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "312 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "313 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "314 -> Linear(in_features=1024, out_features=1024, bias=True)\n",
            "315 -> FairseqDropout()\n",
            "316 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "317 -> Linear(in_features=1024, out_features=4096, bias=True)\n",
            "318 -> Linear(in_features=4096, out_features=1024, bias=True)\n",
            "319 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "320 -> LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "321 -> Linear(in_features=1024, out_features=52004, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.sample(\n",
        "    [\"Policja skontrolowała trzeźwość kierowców\"],\n",
        "    beam=5, sampling=True, sampling_topk=50, sampling_topp=0.95,\n",
        "    temperature=0.95, max_len_a=1, max_len_b=100, no_repeat_ngram_size=3\n",
        ")\n",
        "print(result[0])\n",
        "# Policja skontrolowała trzeźwość kierowców pojazdów. Wszystko działo się na drodze gminnej, między Radwanowem\n",
        "# a Boguchowem. - Około godziny 12.30 do naszego komisariatu zgłosił się kierowca, którego zaniepokoiło\n",
        "# zachowanie kierującego w chwili wjazdu na tą drogę. Prawdopodobnie nie miał zapiętych pasów - informuje st. asp.\n",
        "# Anna Węgrzyniak z policji w Brzezinach. Okazało się, że kierujący był pod wpływem alkoholu. [...]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOQeXJ165PHr",
        "outputId": "badd0dd5-0e4c-42d0-a974-cf0b33b2703b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policja skontrolowała trzeźwość kierowców biorących udział w akcji protestacyjnej w regionie. W związku ze strajkiem policjanci zatrzymali do kontroli aż 937 aut osobowych, w tym trzy ciężarowe. Ponad 1/5 z nich była niesprawna. Zgodnie z zapowiedziami organizatorów akcji, protesty będą trwać przez cały tydzień. Na razie w poniedziałek premier Donald Tusk rozmawiał z przedstawicielami NSZZ Policjantów o udziale w akcji protestacyjnej. Jego zdaniem, protest ma charakter \"głodującego protestu\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.sample(\n",
        "    [\"Premier Donald Tusk rozmawiał z Jarosławem Kaczyńskim\"],\n",
        "    beam=5, sampling=True, sampling_topk=50, sampling_topp=0.95,\n",
        "    temperature=0.95, max_len_a=1, max_len_b=100, no_repeat_ngram_size=3\n",
        ")\n",
        "print(result[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd3XIhlS6-2Y",
        "outputId": "410bbe7e-1315-4373-dd60-6de0754a665d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premier Donald Tusk rozmawiał z Jarosławem Kaczyńskim od 8 do 18 lutego. Rozmowa dotyczyła m.in. sytuacji na Ukrainie, spotkaniach z prezydentem, dymisji rządu i rozpisania przedterminowych wyborów. W kwietniu premier odwołał kilku członków rządu i powołał nowe osoby: szefa kancelarii premiera Michała Dworczyka oraz rzecznika prasowego rządu Pawła Grasia. O zawieszeniu w prawach członków rządu i powołaniu nowych osób poinformował w czwartek szef klubu PO Sławomir Neumann. - Premier odwołał trzech konstytucyjnych ministrów: Mariusza Błaszczaka, Antoniego Macierewicza i Bartłomieja Misiewicza. W nocy\n"
          ]
        }
      ]
    }
  ]
}