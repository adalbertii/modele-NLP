{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4k6CCOqNnUqWM62GwFA0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalbertii/modele-NLP/blob/main/Tokenizacja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizacja**\n",
        "\n",
        "Rozbudowane, duże modele językowe OpenAI działają poprzez konwersję tekstu na tokeny.\n",
        "\n",
        "Modele te uczą się rozpoznawać powiązania statystyczne między tymi tokenami i prześcigują się  w przewidywaniu kolejnego tokenu w sekwencji.\n",
        "\n",
        "Tokenizacja\n",
        "\n",
        "Ciąg może mieć zakres od jednego znaku do słowa lub do kilku słów. W przypadku LLM z dużym oknem kontekstowym można składać całe dokumenty.\n",
        "\n",
        "Tiktoken to tokeniser typu open source firmy OpenAI. Tiktoken konwertuje  sekwencje znaków (zdania) na tokeny; i może ponownie zamienić tokeny z powrotem w zdania.\n",
        "\n",
        "\n",
        "Tokenizacja ciągów\n",
        "\n",
        "    Tokenizacja jest odwracalna i bezstratna, więc można przekonwertować tokeny z powrotem na oryginalny tekst.\n",
        "\n",
        "    Tokenizacja działa na dowolnym tekście.\n",
        "\n",
        "    Tokenizacja kompresuje tekst: sekwencja tokenów jest krótsza niż bajty odpowiadające tekstowi oryginalnemu. W praktyce każdy token odpowiada średnio około 4 bajtom.\n",
        "    \n",
        "    Tokenizacja umożliwia modelom rozpoznawanie wspólnych słów podrzędnych. Na przykład ing jest powszechnym podsłowem w języku angielskim, kodowanie BPE często dzieli kodowanie na tokeny, takie jak encod i ing. Model często widzi token ing w różnych kontekstach, pomagając modelom w uogólnianiu i lepszym zrozumieniu gramatyki.\n",
        "\n",
        "\n",
        "Proces tokenizacji nie jest jednolity we wszystkich modelach; różne modele używają różnych kodowań.\n",
        "\n",
        "Nowsze modele, takie jak GPT-3.5 i GPT-4, wykorzystują odrębny tokenizer w porównaniu ze starszymi modelami GPT-3 i Codex, co skutkuje różnymi tokenami dla tego samego tekstu wejściowego.\n",
        "\n",
        "\n",
        "\n",
        "Różne kodowania tokenów są powiązane z różnymi modelami, dlatego należy o tym pamiętać podczas konwertowania tekstu na tokeny, aby mieć świadomość, jaki model zostanie użyty."
      ],
      "metadata": {
        "id": "umQKgE2C8LVB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyGGvPVJ8K9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmHUkEor5M1I",
        "outputId": "43c6a026-cdd5-4f40-af7e-fa23c98bcdd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n"
      ],
      "metadata": {
        "id": "d7PfJPYn5dQw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
      ],
      "metadata": {
        "id": "2vqW7llM6Wjg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"
      ],
      "metadata": {
        "id": "ykBsQGfh6aEw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"How long is the great wall of China?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac0n4N2K6c3G",
        "outputId": "b00fbad3-4161-45e3-8cb3-5021ff48c730"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# Turn tokens into text with encoding.decode()  #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "encoding.decode([4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J3XCZCRT6p_W",
        "outputId": "99f3369c-d615-4802-c501-0d44e3a40020"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How long is the great wall of China?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22u2GkVK7d1_",
        "outputId": "fae52937-6b58-4e8d-936b-61513a5752f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'How',\n",
              " b' long',\n",
              " b' is',\n",
              " b' the',\n",
              " b' great',\n",
              " b' wall',\n",
              " b' of',\n",
              " b' China',\n",
              " b'?']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "#             Comparing Encodings               #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")\n",
        "\n",
        "compare_encodings(\"How long is the great wall of China?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI4BFmtx7lTe",
        "outputId": "76baf1ce-a86d-4f9c-b047-a76c976a0f30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"How long is the great wall of China?\"\n",
            "\n",
            "r50k_base: 9 tokens\n",
            "token integers: [2437, 890, 318, 262, 1049, 3355, 286, 2807, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n",
            "\n",
            "p50k_base: 9 tokens\n",
            "token integers: [2437, 890, 318, 262, 1049, 3355, 286, 2807, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n",
            "\n",
            "cl100k_base: 9 tokens\n",
            "token integers: [4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n"
          ]
        }
      ]
    }
  ]
}