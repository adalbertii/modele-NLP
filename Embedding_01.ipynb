{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyps0QUaMJCuYVdjl0+RsQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalbertii/modele-NLP/blob/main/Embedding_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8Zm0MLH15R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utworzenie prostego modelu z warstwą Embedding\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=10,output_dim=4,input_length=2)\n",
        "model.add(embedding_layer)\n",
        "model.compile('adam','mse')\n",
        "\n",
        "# input_dim - rozmiar słownika\n",
        "# output_dim - długość wektora dla  każdego słowa\n",
        "# input_length - maksymalna długość sekwencji\n",
        "\n",
        "# W powyższym przykładzie ustawiamy 10 jako rozmiar słownictwa, ponieważ będziemy kodować liczby od 0 do 9.\n",
        "# Chcemy, aby długość wektora słów wynosiła 4, stąd output_dim jest ustawione na 4.\n",
        "# Długość sekwencji wejściowej do warstwy osadzania będzie wynosić 2"
      ],
      "metadata": {
        "id": "POU5qXfQIKfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teraz przekażmy przykładowe dane wejściowe do naszego modelu i zobaczmy wyniki.\n",
        "input_data = np.array([[1,2]])\n",
        "pred = model.predict(input_data)\n",
        "print(input_data.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4pcRPlJXEA",
        "outputId": "316b51ac-0f7a-4708-dbf2-8eec45839be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 145ms/step\n",
            "(1, 2)\n",
            "[[[-0.02334188  0.03078547  0.01017154  0.04552039]\n",
            "  [ 0.00770851 -0.01431287  0.01345507  0.005794  ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widać, każde słowo (1 i 2) jest reprezentowane przez wektor o długości 4. Jeśli wydrukujemy wagi warstwy osadzania, otrzymamy poniższy wynik."
      ],
      "metadata": {
        "id": "40b5QDksJuXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EazLjNyHKFNt",
        "outputId": "5275d63f-0ba1-4eca-d94f-13eaedbe601b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 2, 4)              40        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40 (160.00 Byte)\n",
            "Trainable params: 40 (160.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpa4317yJ1vU",
        "outputId": "5e141673-ce00-4ca7-e475-c430fa9550ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'embedding/embeddings:0' shape=(10, 4) dtype=float32, numpy=\n",
            "array([[ 0.02532529,  0.02959583, -0.02077184,  0.03307004],\n",
            "       [-0.02334188,  0.03078547,  0.01017154,  0.04552039],\n",
            "       [ 0.00770851, -0.01431287,  0.01345507,  0.005794  ],\n",
            "       [-0.04999755,  0.01052343,  0.035213  , -0.03954636],\n",
            "       [ 0.03153438,  0.0402245 ,  0.04568147, -0.03027884],\n",
            "       [ 0.01681492, -0.01277689, -0.02017144,  0.00512385],\n",
            "       [-0.01597724, -0.00012515, -0.02283381, -0.02160322],\n",
            "       [ 0.04674724,  0.0115404 , -0.02743992,  0.00157394],\n",
            "       [ 0.00702375, -0.00607336,  0.00393804, -0.04824213],\n",
            "       [-0.02383035,  0.01415158,  0.03827745, -0.02038814]],\n",
            "      dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te wagi są w zasadzie reprezentacjami wektorowymi słów w słownictwie. Jak omówiliśmy wcześniej, jest to tabela odnośników o rozmiarze 10 x 4, dla słów od 0 do 9. Pierwsze słowo (0) jest reprezentowane przez pierwszy wiersz w tej tabeli, czyli\n",
        "\n",
        "[0.02532529,  0.02959583, -0.02077184,  0.03307004]"
      ],
      "metadata": {
        "id": "MbFD01YXKaGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tym przykładzie nie wytrenowaliśmy warstwy osadzania. Wagi przypisane do wektorów słów są inicjowane losowo."
      ],
      "metadata": {
        "id": "iiYcMjV0K2U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Klasyfikacja recenzji restauracji**\n",
        "\n",
        "\n",
        "Tok działań:    \n",
        "*   Tokenizacja zdań na słowa.\n",
        "*   Utwórzenie zakodowanego wektor\\ one-hot dla każdego słowa.\n",
        "*   Użycie funkcji \"Padding\", w celu upewnienia się, że wszystkie sekwencje mają tę samą długość.\n",
        "*   Przekazanie wypełnionych sekwencji jako danych wejściowwych do warstwy Embedding.\n",
        "*   Spłaszczennie danych do wktora i zastostosowanie  warstwę Dense, aby przewidywać etykietę\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "OZAUMIRlVF3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Embedding,Dense"
      ],
      "metadata": {
        "id": "SGtqray1K1fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby to uprościć, użyjemy łącznie 10 recenzji. Połowa z nich jest pozytywna, reprezentowana przez 0, a druga połowa jest negatywna, reprezentowana przez 1."
      ],
      "metadata": {
        "id": "pEjyX81-WjuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#definicja 10 recencji\n",
        "reviews =[\n",
        "          'Never coming back!',\n",
        "          'horrible service',\n",
        "          'rude waitress',\n",
        "          'cold food',\n",
        "          'horrible food!',\n",
        "          'awesome',\n",
        "          'awesome services!',\n",
        "          'rocks',\n",
        "          'poor work',\n",
        "          'couldn\\'t have done better'\n",
        "]\n",
        "\n",
        "#definicja etykiet (1 0 negatywane, 0- pozytywna)\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "OiQT-Ga8WaZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przyjmiemy rozmiar słownictwa jako 50 i zakodujemy słowa za pomocą funkcji one_hot z Keras."
      ],
      "metadata": {
        "id": "y9ViQAMNu_0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vocab_size = 50\n",
        "encoded_reviews = [one_hot(d,Vocab_size) for d in reviews]\n",
        "print(f'encoded reviews: {encoded_reviews}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up1N-FAjW2em",
        "outputId": "65289c50-cb2b-4ade-9245-a0f17a1c25f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded reviews: [[43, 20, 21], [47, 4], [39, 4], [41, 8], [47, 8], [21], [21, 18], [49], [15, 7], [41, 8, 25, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Widać, że długość każdej zakodowanej recenzji jest równa liczbie słów w tej recenzji. Keras one_hot w zasadzie konwertuje każde słowo na zakodowany indeks one-hot. Teraz musimy zastosować \"padduing\", aby wszystkie zakodowane recenzje miały tę samą długość. Zdefiniujmy 4 jako maksymalną długość i wypełnijmy zakodowane wektory zerami na końcu."
      ],
      "metadata": {
        "id": "KlAF6u8JvnEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='post')\n",
        "print(padded_reviews) #Wypełnione i zakodowane recenzje będą wyglądać następująco."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCsGqQn7v0Ug",
        "outputId": "83dd8323-18ec-47ab-8fe9-b370e0753c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[43 20 21  0]\n",
            " [47  4  0  0]\n",
            " [39  4  0  0]\n",
            " [41  8  0  0]\n",
            " [47  8  0  0]\n",
            " [21  0  0  0]\n",
            " [21 18  0  0]\n",
            " [49  0  0  0]\n",
            " [15  7  0  0]\n",
            " [41  8 25  6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po utworzeniu uzupenionej, zakodowanje  reprezentacji recenzji, jesteśmy gotowi do przekazania jej jako danych wejściowych do warstwy osadzania. W poniższym fragmencie kodu tworzymy prosty model Keras.\n",
        "\n",
        "Ustalimy długość osadzonych wektorów dla każdego słowa na 8, a długość wejściowa będzie maksymalną długością, którą już zdefiniowaliśmy jako 4."
      ],
      "metadata": {
        "id": "tORfLAZlwWjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=Vocab_size,output_dim=8,input_length=max_length)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])"
      ],
      "metadata": {
        "id": "12Tn92XCwlM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcIRcE_swypf",
        "outputId": "a3490b40-b45f-46ac-d86d-51025e1065c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433 (1.69 KB)\n",
            "Trainable params: 433 (1.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proces uczenia zdefiniowanego modelu**"
      ],
      "metadata": {
        "id": "PL6nM8R5w6PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_reviews,labels,epochs=100,verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8foLJOqw_Ez",
        "outputId": "8954fcf1-38ab-4853-9c0d-3c2ed9af5772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c56894f6b60>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po zakończeniu treningu warstwa osadzania nauczyła się wag, które są niczym innym jak reprezentacjami wektorowymi każdego słowa. Sprawdźmy kształt macierzy wag."
      ],
      "metadata": {
        "id": "iqvL6jc9xJTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.get_weights()[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI_6y1jVxOU0",
        "outputId": "f417b6c5-8fb4-4bf0-dbc6-0e42801a7be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jeśli sprawdzimy osadzenie dla pierwszego słowa, otrzymamy następujący wektor.\n",
        "embedding_layer.get_weights()[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLvMro9ExWI7",
        "outputId": "762c2f29-a8f7-4cae-f08b-d35a343efb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.06841037, -0.13783693,  0.12766486, -0.1219708 ,  0.10954367,\n",
              "       -0.07662023, -0.06978109,  0.07932504], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}